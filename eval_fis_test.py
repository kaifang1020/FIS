import os
import re
import json
import math
import numpy as np

from swift.llm import PtEngine, RequestConfig, InferRequest

# ===== 0. ç¯å¢ƒå˜é‡é…ç½® =====
os.environ['MAX_PIXELS'] = '1003520'
os.environ['VIDEO_MAX_PIXELS'] = '50176'
os.environ['FPS_MAX_FRAMES'] = '12'

# ===== 1. è·¯å¾„é…ç½® =====
# â‘  Qwen3-Omni åŸºåº§æ¨¡å‹ç›®å½•ï¼ˆä½ è®­ç»ƒç”¨çš„é‚£ä¸ªï¼‰
BASE_MODEL_DIR = "/gpfsnyu/scratch/km6704/qwen3_omni_30b_a3b"

# â‘¡ LoRA checkpoint ç›®å½•ï¼ˆæ³¨æ„ï¼šæ˜¯å…·ä½“æŸä¸ª checkpointï¼Œæ¯”å¦‚ checkpoint-100ï¼‰
LORA_CKPT_DIR = (
    "/gpfsnyu/scratch/km6704/ondemand/jbnhandsome/train_result_with_thought_baseline/v0-20260110-155034/checkpoint-50"
)
# â‘¢ ä½ åˆšæ‰çš„è¯„æµ‹é›† jsonï¼ˆå°±æ˜¯ä½ è´´å‡ºæ¥é‚£ç§ç»“æ„ï¼‰
EVAL_JSON = (
    "/gpfsnyu/home/km6704/Mr_NT/dataset/fis_filtered_5008_training.json"
)

# â‘£ è¯„æµ‹ç»“æœè¾“å‡ºåˆ°å“ªé‡Œ
RESULTS_PATH = (
    "/gpfsnyu/home/km6704/mswift/ms-swift/fis_test_result_with_CoT_5008.jsonl"
)

# ğŸ¯ è®¾ç½®èµ·å§‹ç´¢å¼•ï¼šä»ç¬¬ 1178 æ¡å¼€å§‹ï¼ˆå³è·³è¿‡ç´¢å¼• 0~1177ï¼‰
START_INDEX = 596

# ===== 2. å·¥å…·å‡½æ•° =====

def parse_score_from_text(gen_text: str):
    """
    ä¿®æ”¹åçš„é€»è¾‘ï¼šåªä» </think> æ ‡ç­¾ä¹‹åçš„æ–‡æœ¬ä¸­æå–ç¬¬ä¸€ä¸ªæ•°å­—ã€‚
    """
    if not gen_text:
        return None

    # å¯»æ‰¾ </think> æ ‡ç­¾ï¼Œå¹¶æˆªå–å…¶åçš„å†…å®¹
    if "</think>" in gen_text:
        # split åå–æœ€åä¸€éƒ¨åˆ†ï¼Œé˜²æ­¢ä¸­é—´å‡ºç°å¹²æ‰°æ ‡ç­¾
        target_text = gen_text.split("</think>")[-1].strip()
    else:
        # å¦‚æœæ¨¡å‹æ²¡æœ‰è¾“å‡º </think> æ ‡ç­¾ï¼Œåˆ™å›é€€åˆ°å¤„ç†å…¨æ–‡ï¼ˆæˆ–è€…æ ¹æ®éœ€æ±‚è¿”å› Noneï¼‰
        target_text = gen_text.strip()

    if not target_text:
        return None

    # åœ¨ç›®æ ‡æ–‡æœ¬ä¸­åŒ¹é…ç¬¬ä¸€ä¸ªæ•°å­—ï¼ˆæ”¯æŒæ­£è´Ÿå·å’Œå°æ•°ï¼‰
    m = re.search(r"[-+]?\d+(\.\d+)?", target_text)
    if not m:
        return None

    try:
        return float(m.group(0))
    except Exception:
        return None


def extract_prompt_and_label(item):
    convs = item["conversations"]
    human = next(c for c in convs if c["from"] == "human")
    gpt   = next(c for c in convs if c["from"] == "gpt")

    prompt = human["value"]
    label_str = gpt["value"]

    try:
        label = float(label_str)
    except Exception:
        label = None

    return prompt, label


def compute_metrics(labels, preds):
    labels = np.array(labels, dtype=float)
    preds = np.array(preds, dtype=float)
    mae = np.mean(np.abs(labels - preds))
    rmse = math.sqrt(np.mean((labels - preds) ** 2))
    if len(labels) > 1 and np.std(labels) > 0 and np.std(preds) > 0:
        corr = np.corrcoef(labels, preds)[0, 1]
    else:
        corr = float("nan")
    return mae, rmse, corr


# ===== 3. ä¸»ç¨‹åº =====

def main():
    print("==== FIS LoRA å¤šæ¨¡æ€è¯„æµ‹ï¼ˆç»­è·‘æ¨¡å¼ï¼‰ ====")
    print(f"èµ·å§‹ç´¢å¼• : {START_INDEX}")
    print(f"ç»“æœè·¯å¾„ : {RESULTS_PATH}")

    # --- [Step 1] åŠ è½½å·²æœ‰çš„ç»“æœï¼Œç”¨äºåŒæ­¥å…¨é‡æŒ‡æ ‡ ---
    all_labels = []
    all_preds = []
    if os.path.exists(RESULTS_PATH):
        print(f"æ­£åœ¨è¯»å–å·²æœ‰æ–‡ä»¶ä»¥åŠ è½½å‰ {START_INDEX} æ¡çš„æŒ‡æ ‡...")
        with open(RESULTS_PATH, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    try:
                        data = json.loads(line)
                        if data.get('pred') is not None:
                            all_labels.append(data['label'])
                            all_preds.append(data['pred'])
                    except:
                        continue
        print(f"å·²åŠ è½½ {len(all_preds)} æ¡æœ‰æ•ˆå†å²é¢„æµ‹ã€‚")

    # --- [Step 2] åŠ è½½æ¨ç†å¼•æ“ ---
    print("\n[Step 2] åŠ è½½ PtEngine + LoRA ...")
    engine = PtEngine(
        BASE_MODEL_DIR,
        adapters=[LORA_CKPT_DIR],
        max_batch_size=1,
    )

    request_config = RequestConfig(
        max_tokens=1024,
        temperature=0.0,
    )

    print("\n[Step 3] åŠ è½½ eval æ•°æ®é›†...")
    with open(EVAL_JSON, "r", encoding="utf-8") as f:
        dataset = json.load(f)
    print(f"æ•°æ®é›†æ€»è§„æ¨¡: {len(dataset)}")

    # ğŸ¯ ä½¿ç”¨ "a" æ¨¡å¼æ‰“å¼€æ–‡ä»¶ï¼Œæ–°ç»“æœå°†è¿½åŠ åœ¨æœ«å°¾
    os.makedirs(os.path.dirname(RESULTS_PATH), exist_ok=True)
    fw = open(RESULTS_PATH, "a", encoding="utf-8")

    print(f"\n[Step 4] å¼€å§‹æ¨ç†ï¼ˆä» idx={START_INDEX} å¼€å§‹ï¼‰...\n")
    
    for idx, item in enumerate(dataset):
        # ğŸ¯ è·³è¿‡é€»è¾‘
        if idx < START_INDEX:
            continue

        video_path = item.get("video", "")
        prompt, label = extract_prompt_and_label(item)

        if label is None:
            print(f"[WARN] idx={idx} label è§£æå¤±è´¥ï¼Œè·³è¿‡ã€‚")
            continue

        videos_arg = [video_path] if os.path.exists(video_path) else None

        infer_req = InferRequest(
            messages=[{"role": "user", "content": prompt}],
            videos=videos_arg,
        )

        try:
            resp_list = engine.infer([infer_req], request_config)
            gen_text = resp_list[0].choices[0].message.content
        except Exception as e:
            print(f"[ERROR] idx={idx} æ¨ç†æŠ¥é”™ï¼š{repr(e)}")
            gen_text = ""
        
        # ğŸ¯ æå– </think> ä¹‹åçš„åˆ†æ•°
        score = parse_score_from_text(gen_text)
        print(f"[{idx+1}/{len(dataset)}] Score: {score} | GT: {label}")

        record = {
            "idx": idx,
            "video": video_path,
            "label": label,
            "raw_text": gen_text,
            "pred": score,
        }
        fw.write(json.dumps(record, ensure_ascii=False) + "\n")
        fw.flush()

        if score is not None:
            all_labels.append(label)
            all_preds.append(score)

    fw.close()
    print(f"\nè¯„æµ‹å®Œæˆï¼Œç»“æœå·²è¿½åŠ è‡³ï¼š{RESULTS_PATH}")

    # ===== 5. è®¡ç®—æœ€ç»ˆå…¨é‡æŒ‡æ ‡ =====
    if len(all_labels) > 0:
        mae, rmse, corr = compute_metrics(all_labels, all_preds)
        print("\n====== FIS å…¨é‡æ±‡æ€»æŒ‡æ ‡ï¼ˆå†å² + æ–°è·‘ï¼‰ ======")
        print(f"æœ‰æ•ˆæ ·æœ¬æ€»æ•°: {len(all_labels)}")
        print(f"MAE : {mae:.4f}")
        print(f"RMSE: {rmse:.4f}")
        print(f"Pearson corr: {corr:.4f}")

if __name__ == "__main__":
    main()